#' @export
#' @title make a multicore parallelized mlr benchmark experiment for various learners on multiple tasks
#' @author Thomas Goossens
#' @import mlr
#' @import parallelMap
#' @importFrom magrittr %>%
#' @param cpus an integer specifying the number of cpus to use for the benchamrk. Default is 1
#' @param tasks a list which elements are object of class mlr::makeRegrTask()
#' @param learners a list which elements are object of class mlr::makeLearner()
#' @param measures a list of the mlr performance metrics you want to get. Default is list(rmse)
#' @param keep.pred a boolean specifying if you want to keep the bmr preds. defaut = TRUE
#' @param models a boolean specifying if you want to keep the bmr models. defaut = FALSE
#' @param grouping a numeric specifying the number of tasks you want to benchamrk in a single batch. Default to 1000
#' @param level a character specifying the paralelllization level. Default  = "mlr.benchmark"
#' @param resampling  a character specifying the type of mlr's Cross-Validation strategy. Default = LOO
#' @param output_dir a character specifying the path of an exising directory where you want to save the bmr outpus.
#' @param prefix a character specifying the prefix you want to use for the bmr file names.
#' @param removeTemp a boolean specifying if the temporary .rds generated by the function must be deleted at the end of the process.
#' Default to FALSE
#' @return a list which elements are lists

makeBatchOfBenchExp <- function(
  tasks,
  learners,
  measures = list(rmse),
  keep.pred = TRUE,
  models = FALSE,
  grouping = 1000,
  level = "mlr.benchmark",
  resamplings = "LOO",
  cpus = 4,
  prefix = "",
  output_dir = "bmrs",
  removeTemp = FALSE){

  output = list(value = NULL, condition = list(type = NULL, message = NULL))
  snitch = FALSE

  doBenchmark = function(){

    # set seed to make bmr experiments reproducibles
    set.seed(1985)

    # hack for tasks length when only a single task
    # ::todo::

    # split tasks in multiple subgroups to avoid memory saturation
    if (length(tasks) <= grouping) {
      # warning("AgrometeoR warning : length of your tasks list smaller than your grouping parameter.
      # Setting the grouping value equal to the length of your tasks. ")
      grouping = length(tasks)
    }

    tasks.groups.start = seq(from = 1, to = length(tasks), by = grouping)
    tasks.groups.end = seq(from = grouping, to = length(tasks), by = grouping)

    # conducting the bmrs by subgroups and writting results to temp .rds file

    # https://r4ds.had.co.nz/iteration.html#the-map-functions
    # bmrs = tasks %>% map(possibly(mlr::benchmark, NULL))

    lapply(seq_along(as.list(tasks.groups.start)),
      function(x) {

        # message
        message(paste0(
          "Conducting batch of benchmark experiments for tasks " ,
          tasks.groups.start[x], "-",
          tasks.groups.end[x]))

        # hack to avoid wrong last task number
        if (is.na(tasks.groups.end[x])) {tasks.groups.end[x] = tasks.groups.start[x]}

        # enable parallelization with level = mlr.resample
        if (cpus > 1) {
          parallelMap::parallelStart(mode = "multicore", cpus = cpus, level = level)
        }

        #####
        ## explorative code in case https://stackoverflow.com/questions/55608882/how-to-make-the-benchmark-function-not-to-fail-if-a-specific-learner-fails-on-a
        # [1] "AgrometeoR::makeBatchOfBenchExp raised an error -> Error in stopWithJobErrorMessages(inds, vcapply(result.list[inds], as.character)): Errors occurred in 1 slave jobs, displaying at most 10 of them:\n\n00499: Error in load.variogram.model(object$model[[name]], c(i - 1, i - 1), max_dist = max_dist) : \n  variogram range can never be negative\n\n\n"
         #  resample_by_task = function(t) {
         #    print(keep.pred)
         #   learners %>% purrr::map(possibly(
         #     ~ mlr::resample(.,
         #       task = t,
         #       resampling = mlr::makeResampleDesc(resamplings),
         #       measures = measures,
         #       keep.pred = keep.pred,
         #       models = models), NULL)
         #   )
         # }
         #
         # bmrs = purrr::map(tasks[tasks.groups.start[x]:tasks.groups.end[x]], ~resample_by_task(.))
        #####

        tasks = tasks[tasks.groups.start[x]:tasks.groups.end[x]]

        bmrs = tasks %>%
          purrr::map(possibly(~mlr::benchmark(.,
            learners = learners,
            resamplings = mlr::makeResampleDesc(resamplings),
            measures = measures,
            keep.pred = keep.pred,
            models = models), NULL))

        # stop the parallelized computing
        if (cpus > 1) {
          parallelMap::parallelStop()
        }

        # save the bmr object to a file
        saveRDS(object = bmrs, file = paste0(output_dir,
          "/",
          prefix,
          "_bmr_",
          mlr::getTaskId(tasks[[tasks.groups.start[x]]]),
          "_",
          mlr::getTaskId(tasks[[tasks.groups.end[x]]]),
          ".rds"))

        # remove the bmrs object stored in RAM
        rm(bmrs)

        # success message
        message(paste0(
          "Results of batch of Benchmark experiments for tasks " ,
          mlr::getTaskId(tasks[[tasks.groups.start[x]]]), " - ",
          mlr::getTaskId(tasks[[tasks.groups.end[x]]]), " conducted and written to file. "))

      })


        # loading all the temp bmr files and merging in a single big bmr object
        bmr_files = list.files(path = output_dir, pattern = prefix, full.names = TRUE)
        bmrs = map(bmr_files, readRDS)

        # deleting temporary .rds files is removeTemp = true
        if (isTRUE(removeTemp)) {
          file.remove(bmr_files)
        }


        # extracting only the good ones (task without failure)
        bmrs = bmrs %>% purrr::flatten()
        bmrs = bmrs %>% purrr::compact()

        if (length(bmrs) > 1) {
          bmrs = mergeBenchmarkResults(bmrs)
          warning("Some bmr results were excluded become of possible learner failure")
        }
        else {bmrs = bmrs[[1]]}

        # Throw a success message
        message("Success, batch of benchmark experiment conducted")

        return(bmrs)
  }

  tryCatch(
    expr = {
      # check if output dir exists
      stopifnot(dir.exists(output_dir))

      # in case everything went fine, do makeBatchOfBenchExp
      output$value = doBenchmark()
      output$condition$type = "success"
      output$condition$message = "benchmarks conducted"
      snitch = TRUE

    },
    warning = function(w){
      warning = paste0(
        "AgrometeoR::makeBatchOfBenchExp raised a warning -> ",
        w)
      snitch <<- TRUE
      output$value <<- doBenchmark()
      output$condition$type <<- "warning"
      output$condition$message <<- warning
    },
    error = function(e){
      error = paste0(
        "AgrometeoR::makeBatchOfBenchExp raised an error -> ",
        e)
      output$condition$type <<- "error"
      output$condition$message <<- error
    },
    finally = {
      finalMessage = paste0(
        "makeBatchOfBenchExp has encountered : ",
        output$condition$type,
        ". \n",
        "All done with makeBatchOfBenchExp "
      )
      message(finalMessage)
      return(list(snitch = snitch, output = output))
    }
  )
}




