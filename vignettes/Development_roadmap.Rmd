---
title: "Development Roadmap"
author: "Thomas Goossens"
date: "02/05/2019"
output: html_document
bibliography: ../inst/bibliography/bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
library(mlr)
library(tidyr)
library(dplyr)
library(stringr)
```

# Introduction

The tool to assess which spatialization method is the most appropriate is *a batch of benchmark experiments performed on multiple set of records from an historical dataset of observed weather data recorded by the stations of interest where the predictive performance of various learners are assessed on each set of records by an iterated leave-one-out cross validation*. 

This approach relies heavily on the [mlr package](https://mlr.mlr-org.com/) [@bischl_mlr:_2016] which is an unified interface to perform machine learning analysis using R.

We have decided to use data from 01 jan 2016 to 31 dev 2017 as these two years cover to very distinct situations (2016 = wet and 2017 = dry). To conduct these experiments we will use the `makeBenchmark` function integrated with this package. Many parameters can influence the quality of the spatial predictions and these will be tested in multiple benchmark experiments. This is why the exploration field must be restricted and the investigated parameters must be prioritized. This article present our investigations roadmap. Our global philosophy is to start with a simple approach and gradually add complexity to it. 

# Goals 

We must assess which is the best spatialization technique for :

* air temperature (TSA) and relative humidity (HRA)
* for both hourly and daily data.

We don't need to asses the best spatialization technique for :

* Leaves wetness (HCT) as it will be computed from already spatialized datasets (HRA and ::TODO::).
* rainfall (PLU) as we receive spatialized datasets from RMI's rainfalls radar ::TODO:: (name of the product ?).

# Terminology

Here we define the terminology that will be used in this document. This terminology is important as we need precise definitions to avoid confusions in our future interpretations and discussions. As stated earlier, we will need to conduct :

*a batch of benchmark experiments performed on multiple set of records from an historical dataset of observed weather data recorded by the stations of interest where the predictive performance of various learners are assessed on each set of records by an iterated leave-one-out cross validation*
 
* __Stations of interest__ : Weather stations considered for the spatilization process. 
* __Set of records__ : a set of data recorded by the stations of interests at a specific moment. 
* __Historical dataset__ : the dataset containing all the hourly set of records of weather parameters for all the stations of interest.
* __Iterated leave-one-out cross validation__ : cross validation of a model created by training a specific learner on a single set of records from the historical dataset
* __Benchmark Experiment__ : comparison of multiples iterated leave-one-out cross validations results operated on the same set of records but with different learners
* __Batch of benchmarks experiments__ : set of benchmark experiments conducted on multiple set of records.

# Exploration field

Here we present all the __exploration parameters__ that can influence the results of a batch of benchmark experiments.

## Stations of interest

It is important to get a deep insight and comprehensive overview of our weather station network before interpolating its data in order to avoid the integration of non-desired local or structural effects during the interpolation process.

A specific attention will be ported on the analysis of the quality of the data produced by each of our stations. We will need to carry an analysis in order to detect eventual structural or local effects such as overheating in temperature shelters. 

Local temperature effects will be detectable by pointing out abnormally high our low values appearing from long term analysis of each of the stations from our network. Again, a good knowledge of the station network (eg : situation and direct environment of each of the stations) is required. To remove local effects from the interpolation process, each station could first be weighted according to a quality parameter characterized by the local situation of the considered station. Time series analysis ( [example map](https://pokyah.github.io/pokyah-maps/temperature/) ::TODO:: find source code and create dedicated vignette) will help us for this purpose.

The Agromet project aims to spatialize weather data gathered both by the [Pameseb](https://www.pameseb.be/) network owned by the [CRA-W](http://www.cra.wallonie.be/fr) and stations owned by the national weather office [RMI](https://www.meteo.be). 

Before integrating two different networks in the spatilization process, we need to assess their intercompatibilty. To address this, both our team and the RMI works on an intercomparison of the networks performed by the mean of a location (Humain - Belgium) equiped with 2 stations belongings to the 2 networks. The first results of this comparative analysis are available on [this repository](https://pokyah.github.io/AWS-Humain-comparison/).

::TODO:: maps of the 2 networks

We will investigate if combining the two networks improve the quality of the predictions. Also we will consider if correcting the Pameseb TSA data using the correction model built by the Humain stations intercomparison (::TODO:: see article) increases the quality of the predictions. 

* Pameseb
* Pameseb + RMI
* Pameseb corr + RMI

## Time resolutions

The spatialized data must be available for two different time resolutions :

* __Daily__
* __Hourly__

As the performance of the same learner might differ from one time resolution to the other, we need to investigate both time resolutions.

## Explanatory variables - features

These variables must already be available as spatialized data in order to be used as explanatory variables (also called ___features__). We can make the distinction between the __static explanatory variables__ and the __dynamic explanatory variables__. The static variables are constant over time while the dynamic vary over time.

The static explanatory variables to investigate are : 

* __latitude and longitude__
* __elevation__ : the dataset is freely available on the [Registry of Open Data on AWS](https://registry.opendata.aws/terrain-tiles/) and can be easily donwloaded using the [elevatr](https://github.com/jhollist/elevatr) package 
* __slope__ : this variable is computed from elevation using the [raster package](https://CRAN.R-project.org/package=raster) [@hijmans_package_2015]
* __aspect__ : this variable is computed from elevation using the [raster package](https://CRAN.R-project.org/package=raster)
* __soil occupation__ : this variable is downloaded form the [NGI website](http://inspire.ngi.be/download-free/atomfeeds/AtomFeed-en.xml) ::TODO:: details

The investigated dynamic explanatory variables are :

* __irradiance__ (ENS) : data pulled from the [MSG Downward Surface Shortwave Flux (MDSSF)](https://landsaf.ipma.pt/en/products/longwave-shortwave-radiation/dssf/) from Landsaf [@trigo_satellite_2011]
* __INCA_BE analysis T0__ (INC) : data pulled from the [INCA-BE operational nowcasting system of the RMI](http://radar.meteo.be/en/2730756-Operational+nowcasting.html) [@reyniers_nowcasting_2012]

## Algorithms

Numerous regression algorithms exist. We have decided to restrict the investigation field to the one that have already proven their efficiency in other studies.

A __learner__ is an implementation of an algorithm for which a filtering of the explanatory variables to use might be applied and where the hyper-parameters values to test are set.

We have decided to group the algorithms in various categories of increasing complexity. The source code used to construct the learners from these algorithms is stored into the `data-raw/makeLearners.R` file of the present package. These learners are constructed on the basis of the `mlr` package and are available once you have called `library(agrometeoR)`

### Base algorithms

These are those tested by the [ZEPP](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2338.2007.01134.x/abstract) [@zeuner_use_2007]. 

* multiple linear regression
* inverse distance weighted
* one nearest neighbour
* kriging

### Other algorithms

* splines degree 1, 2 and 3
* multiple nearest neighbours

### explorative algorithms

These are some of the algorithms tested in the [Kilimanjaro study](https://www.sciencedirect.com/science/article/pii/S2211675315000482) [@appelhans_evaluating_2015] : 

* generalized linear model (glm),
* cubist
* artificial neural network

### Deep Learning

This approach will be based on the [TensorFlow](https://tensorflow.rstudio.com/) library but will not be considered before 2020. A good introduction to deep learning with R is available in the [Machine Learning with R and TensorFlow video](https://www.youtube.com/watch?v=atiYXm7JZv0&t=73s).

# Milestones

::TODO:: explain the incrementability of these bmrs experiments

Multiple combinations of the described __exploration parameters__ will be tested in various batches of benchmarks experiments. Each of these batches represent one of our milestones. Here below we present all of the  __base milestones__ (i.e. constructed on the basis of the base learners) resulting from all the possible combinations we have decided to investigate. 

As different target variables can be explained by different explanatory variables, we refer to different milestones numbers for each of the target variables even if in some cases a milestone might be constructed from the same combination of parameters.

From a deployment strategy point of view, once a milestone has been benchmarked, the idea is to use its best learner on the production server that delivers the spatialized data. The produced data must then be flagged by a combination of the milestone number and algorithm abbrevitation.


spatialis&é sur base d'un résultat de bmr milestone en utlisant le meilleur learner de cette mielstone
## TSA milestones

### milestoneTsaH1

```{r, milestoneTsa1, echo = FALSE}
algorithms = c('mulLR','IDW','1NN', 'OK', 'KED')
features = c("alt, lon, lat", "lon, lat", "lon, lat", "lon, lat", "alt, lon, lat")
hyperparameters = c(NA, NA, NA, "range = 800, psill = 200000, model.manual = 'Sph', nugget = 0", "range = 800, psill = 200000, model.manual = 'Sph', nugget = 0")
stations = c("Pameseb", "Pameseb", "Pameseb", "Pameseb", "Pameseb")
resolution = c("hourly", "hourly", "hourly", "hourly", "hourly")
target = "tsa"

milestoneTsaH1 = data.frame(algorithms, features, hyperparameters, stations, resolution, target)
knitr::kable(milestoneTsaH1)
```

### milestoneTsaD1

```{r, milestonetsaD1, echo = FALSE}
milestoneTsaD1 = milestoneTsaH1 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily"))

knitr::kable(milestoneTsaD1)
```

### milestoneTsaH2

```{r, milestoneTsaH2, echo = FALSE}
milestoneTsaH2 = milestoneTsaH1 %>%
  dplyr::filter(algorithms %in% (c("OK", "KED")))
milestoneTsaH2$hyperparameters = c("range = 400, psill = 100000, model.manual = 'Sph', nugget = 0", "range = 400, psill = 100000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH2)
```

### milestoneTsaD2

```{r, milestoneTsaD2, echo = FALSE}
milestoneTsaD2 = milestoneTsaH2 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 

knitr::kable(milestoneTsaD2)
```

### milestoneTsaH3

```{r, milestoneTsaH3, echo = FALSE}
milestoneTsaH3 = milestoneTsaH2
milestoneTsaH3$hyperparameters = c("range = 1600, psill = 200000, model.manual = 'Sph', nugget = 0", "range = 1600, psill = 800000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH3)
```

### milestoneTsaD3

```{r, milestoneTsa6, echo = FALSE}
milestoneTsaD3 = milestoneTsaD2 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 

knitr::kable(milestoneTsaD3)
```

### milestoneTsaH4

```{r, milestoneTsaH4, echo = FALSE}
milestoneTsaH4 = milestoneTsaH1 %>%
  dplyr::filter(algorithms %in% (c("mulLR", "KED"))) %>%
  dplyr::mutate(
    features, features = str_replace(features, "alt, lon, lat", "alt, lon, lat, incaA")) 
knitr::kable(milestoneTsaH4)
```

### milestoneTsaD4

```{r, milestoneTsaD4, echo = FALSE}
milestoneTsaD4 = milestoneTsaH4 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD4)
```

### milestoneTsaH5

```{r, milestoneTsaH5, echo = FALSE}
milestoneTsaH5 = milestoneTsaH4 %>%
  dplyr::filter(algorithms %in% c("KED"))
milestoneTsaH5$hyperparameters = c("range = 400, psill = 100000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH5)
```

### milestoneTsaD5

```{r, milestoneTsaD5, echo = FALSE}
milestoneTsaD5 = milestoneTsaH5 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD5)
```

### milestoneTsaH6

```{r, milestoneTsaH6, echo = FALSE}
milestoneTsaH6 = milestoneTsaH5
milestoneTsaH6$hyperparameters = c("range = 1600, psill = 200000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH6)
```

### milestoneTsaD6

```{r, milestoneTsaD6, echo = FALSE}
milestoneTsaD6 = milestoneTsaH6 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD6)
```


######### 

#########

### milestoneTsaH7

```{r, milestoneTsaH7, echo = FALSE}
algorithms = c('mulLR','IDW','1NN', 'OK', 'KED')
features = c("alt, lon, lat", "lon, lat", "lon, lat", "lon, lat", "alt, lon, lat")
hyperparameters = c(NA, NA, NA, "range = 800, psill = 200000, model.manual = 'Sph', nugget = 0", "range = 800, psill = 200000, model.manual = 'Sph', nugget = 0")
stations = c("Pameseb + RMI", "Pameseb + RMI", "Pameseb + RMI", "Pameseb + RMI", "Pameseb + RMI")
resolution = c("hourly", "hourly", "hourly", "hourly", "hourly")
target = "tsa"

milestoneTsaH7 = data.frame(algorithms, features, hyperparameters, stations, resolution, target)
knitr::kable(milestoneTsaH7)
```

### milestoneTsaD7

```{r, milestonetsaD8, echo = FALSE}
milestoneTsaD7 = milestoneTsaH7 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily"))

knitr::kable(milestoneTsaD7)
```

### milestoneTsaH8

```{r, milestoneTsaH8, echo = FALSE}
milestoneTsaH8 = milestoneTsaH7 %>%
  dplyr::filter(algorithms %in% (c("OK", "KED")))
milestoneTsaH8$hyperparameters = c("range = 400, psill = 100000, model.manual = 'Sph', nugget = 0", "range = 400, psill = 100000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH8)
```

### milestoneTsaD8

```{r, milestoneTsaD8, echo = FALSE}
milestoneTsaD8 = milestoneTsaH8 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 

knitr::kable(milestoneTsaD8)
```

### milestoneTsaH9

```{r, milestoneTsaH9, echo = FALSE}
milestoneTsaH9 = milestoneTsaH8
milestoneTsaH9$hyperparameters = c("range = 1600, psill = 200000, model.manual = 'Sph', nugget = 0", "range = 1600, psill = 800000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH9)
```

### milestoneTsaD9

```{r, milestoneTsaD9, echo = FALSE}
milestoneTsaD9 = milestoneTsaH9 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 

knitr::kable(milestoneTsaD9)
```

### milestoneTsaH10

```{r, milestoneTsaH10, echo = FALSE}
milestoneTsaH10 = milestoneTsaH7 %>%
  dplyr::filter(algorithms %in% (c("mulLR", "KED"))) %>%
  dplyr::mutate(
    features, features = str_replace(features, "alt, lon, lat", "alt, lon, lat, incaA")) 
knitr::kable(milestoneTsaH10)
```

### milestoneTsaD10

```{r, milestoneTsaD10, echo = FALSE}
milestoneTsaD10 = milestoneTsaH10 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD10)
```

### milestoneTsaH11

```{r, milestoneTsaH11, echo = FALSE}
milestoneTsaH11 = milestoneTsaH10 %>%
  dplyr::filter(algorithms %in% c("KED"))
milestoneTsaH11$hyperparameters = c("range = 400, psill = 100000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH11)
```

### milestoneTsaD11

```{r, milestoneTsaD11, echo = FALSE}
milestoneTsaD11 = milestoneTsaH11 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD11)
```

### milestoneTsaH12

```{r, milestoneTsaH12, echo = FALSE}
milestoneTsaH12 = milestoneTsaH11
milestoneTsaH12$hyperparameters = c("range = 1600, psill = 200000, model.manual = 'Sph', nugget = 0")

knitr::kable(milestoneTsaH12)
```

### milestoneTsaD12

```{r, milestoneTsaD12, echo = FALSE}
milestoneTsaD12 = milestoneTsaH12 %>%
  dplyr::mutate(
    resolution, resolution = str_replace(resolution, "hourly", "daily")) 
knitr::kable(milestoneTsaD12)
```





## Future milestones

As shown in the previous tables, the hyper-parameters values for the kriging learners KED and OK have been preset. No [hyper-parameter tuning](https://mlr.mlr-org.com/articles/tutorial/tune.html) is conducted in order to assess their values. This is a choice that was decided with the project steering committee in order to reduce computing time and to make things simple at the beginning. The combinations of the hyper-parameters values are based on what RMI uses for their own maps (personnal communication by M. Journée).

In a next future, it might be interesting to use an hyper-tuning loop in order to find the best-hyperparameters values to use. Note that this is a very computationnaly intensive step.

# References



